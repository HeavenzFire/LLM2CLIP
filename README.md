# LLM2CLIP: Extending the Capability Boundaries of CLIP through Large Language Models
## Introduction
This is the official implementation of LLM2CLIP. LLM2CLIP aims to embrace the power of LLMs to unlock CLIP’s potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer’s textual discriminability.

## News 🚀🚀🚀
## Model Zoo (Coming Soon) 
## 💻 How to Install
```
conda create -n llm2clip python=3.8
conda activate llm2clip

pip install -r requirements.txt
```
### Data Preparation (Coming Soon) 
### 🔥 Training  
```sh run.sh```

## ❤️ Acknowlegement

Our code is built on top of [eva-clip](https://github.com/baaivision/EVA/tree/master/EVA-CLIP). Thanks for their nice work!